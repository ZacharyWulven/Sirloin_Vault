---
layout: post
title: 03-Embeddings 和向量数据库
date: 2025-07-29 16:45:30.000000000 +09:00
categories: [AI, AIStarted]
tags: [AI, AIStarted]
---


## Embeddings
* Embeddings 和向量数据库：用于 RAG 知识库


### 传统语义特征计算方法
#### 为酒店建立内容推荐系统

* 基于用户选择的酒店，推荐相似度高的 Top10 个其他酒店
    - 方法：计算当前酒店（描述信息）特征向量与整个酒店特征矩阵的余弦
    
    
![image](/assets/img/ai/start/e_x.png)


* 句子 A 向量：[1, 1, 2, 1, 1, 1, 0, 0]
* 句子 B 向量：[1, 1, 1, 0, 1, 2, 1, 1]


![image](/assets/img/ai/start/e_x2.png)

    
> 向量就是在某些特征上表达
{: .prompt-info }


* 余弦相似度：
    - 通过测量两个向量的夹角的余弦值来度量它们之间的相似性。
    - 判断两个向量⼤致方向是否相同，方向相同时，余弦相似度为 1；两个向量夹角为 90° 时，余弦相似度的值为 0，方向完全相反时，余弦相似度的值为 -1。
    - 范围：两个向量之间夹角的余弦值为 [-1, 1]；1 为完全一致，-1 为完全相反

     
> 分词：将句子拆分为分词
{: .prompt-info } 
    
   
### N-Gram：N 元语法
* 上边对两个句子向量特征问题？
    - 问题在于对特征的理解过于简单了，只看有还是没有，而没有考虑顺序等
    - 上边只是一种维度，即 `N=1 unigram`, 称为一元语法


* N-Gram：N 元语法:
    - 基于一个假设：第 n 个词出现与前 n-1 个词相关，而与其他任何词不相关
    - N=1 unigram; N=2 bigram; N=3 trigram
    - N-Gram 指的是给定一段文本，其中的 N 个 item 的序列
    - 比如文本：A B C D E，对应的 Bi-Gram（组合）为 A B, B C, C D, D E
    - 当一阶特征不够用时，可以用 N-Gram 做为新的特征。比如在处理文本特征时，一个关键词是一个特征，但有些情况不够用，需要提取更多的特征


> 采用 N-Gram => 可以理解是相邻两个关键词的特征。如何了解事物的特征表达？N-Gram 就是最基本的一种方式
{: .prompt-info }



### TF-IDF 
* `TF-IDF = TF * IDF`

* TF:（Term Frequency）, `词频：TF = 单词次数 / 文档中总单词数`
    - 一个单词的重要性和它在文档中出现次数呈比例
    - 例如：`hotel` 单词在所有酒店描述中都出现过，那个这个 `hotel` 就不重要了

* IDF（Inverse Document Frequency）：逆向文档频率，即可理解为区分度、重要性
    - 一个单词在文档中的区分度。这个单词出现在文档中次数越少，区分度越大，IDF 越大
    - 公式：`IDF = log(文档总数 / (单词出现的文档数 + 1))`


![image](/assets/img/ai/start/h.png)


### 推荐系统原理

#### 基于内容的推荐：
* Step1：向量化，即对酒店描述（每个商品）进行特征提取
    - N-Gram：提取 N 个连续字的集合，作为特征的元素
    - TF-IDF：按照 (min_df, max_df) 提取关键词，并生成 TFIDF 矩阵

* Step2：预选计算酒店之间的相似度矩阵
    - 余弦相似度

* Step3：对于指定的酒店，选择相似度最大的 Top-K 个酒店进行输出


> 两者像不像是看向量之间的对比，向量是特征上的表达。中文分词器：Python 库 jieba
{: .prompt-info } 


## Word Embedding (Google 推出)

### 思考：试想一下上边酒店推荐有 26879 维度特征
* N-Gram + TF-IDF 的特征表达会让特征矩阵非常稀疏（很多值为 0），计算量大，有没有更适合的方式？
* 有没有更好的方式让它不稀疏，答案是 `Embedding` 


### 什么是 Embeddings
* `Embedding` 是一种降维度策略，将不同特征转换为维度相同的向量
    - 压缩到一个向量中，使得数据不为 0 更容易计算。向量是通用技术，万事万物都可向量化。也可对人进行 Embeddings



![image](/assets/img/ai/start/e_word.png)



### Word Embedding 例子

![image](/assets/img/ai/start/w_e.png)


![image](/assets/img/ai/start/w_e2.png)


* 每个单词都是 50 维，有意思是 `king - man + woman 与 queen 近似`


#### Embedding 工具：Word2Vec
* 这个工具是训练出来的
* 理论：
    - 通过 Embedding，把原先词所在空间映射到一个新的空间中去，使得语义上相似的单词在该空间内距离相近。
    - Word Embedding => 学习隐藏层的权重矩阵
    - 输入测是 one-hot 编码
    - 隐藏层的神经元数量为 hidden_size（Embedding Size）
    - 对于输入层和隐藏层之间的权值矩阵 W， 大小为 [vocab_size, hidden_size]
    - 输出层为 [vocab_size] 大小的向量，每一个值代表着输出一个词的概率

* 通俗易懂解释：
    - 把特征压缩，以前特征有和没有用 `1/0` 表达, 
    - 例如大不列颠百科全书：这本书假如一共一万个单词，TF 词频铺开，有就为 1，没有就为 0，
    - 对于 king 这个词，某一位为 1，其他都是 0，这时一万维中绝大部分是 0，数据很稀疏
    - 那么可想办法从一万维度压缩（Embedding）到 300 维度，减少 0 方便后边计算
    - 虽然 Embedding 压缩可能造成信息丢失，但可用到原来 3% 的存储空间占到 99% 的信息量，这样收益较大
    
    
* Word2Vec 两种模式：让其预测一个单词两边的邻居是什么? 或猜邻居的概率

* Word2Vec Embedding => 1 万维度到 300 维度

![image](/assets/img/ai/start/e_h.png)



#### 训练词向量工具 Gensim

```shell
$ pip install gensim
```

* 可以从非结构化文本中，无监督地学习到隐层的主题向量表达
* 每一个向量变换的操作都对应着一个主题模型
* 支持 TF-IDF，LDA, LSA, word2vec 等多种主题模型算法
* 使用方式：
    - 建立词向量模型：word2vec.Word2Vec(sentences) window 句子中当前单词和被预测单词的最大距离 
    - min_count 需要训练词语的最小出现次数，默认为 5 
    - size 向量维度默认为 100
    - worker 训练使用的线程数，默认为 1 即不使用多线程
    - 模型保存 model.save(fname)
    - 模型加载 model.load(fname)


> 这个工具使得计算机对单词是用向量的方式去理解的
{: .prompt-info } 


* 如果训练的好，最后结果可能会有些规律，例如前边的 `king - man + woman 与 queen 近似`

> 小结：隐藏层就是 Embeeding，可以理解为是一个向量字典。用神经网络压缩到你设计的网络结构（以 Word2Vec 为例，即 300 维度的隐藏层），让它去学习然后完成输出任务。原来是一万维度输入 => 输出层。`Embeeding` 后是基于 300 维度的隐藏层 => 输出层
{: .prompt-info } 



> 计算机背后的`压缩的特征` 即 `Embeeding`
{: .prompt-info }


## Embeeding 模型选择

* Embedding 模型将文本等离散数据转换为低维、稠密的向量，捕捉其语义信息。向量空间中的距离（如余弦相似度）可反映文本间的语义相似度。
    - 维度：2 的 N 次方
* Embedding 模型强度远远小于大模型，它只是压缩信息到一个空间中，它没有做生成式的 AI 


> Embedding：即把一个信息（例如单词）向量化，进行数据抽取。即把数据放到一个密闭空间中（大体积 => 小体积），方便存储和管理信息。
`Embedding 的本质也是语义的相似度`
{: .prompt-info }


#### [MTEB](https://huggingface.co/spaces/mteb/leaderboard) (Massive Text Embedding Benchmark) 
* 是一个全面的评测基准，它涵盖了分类、聚类、检索、排序等 8 大类任务和 58 个数据集。
    - 检索 (Retrieval)：从一个庞大的文档库中，根据用户输入的`查询（Query）`(相似度查找)，找出最相关的文档列表
    - 语义文本相似度 (Semantic Textual Similarity, STS)：判断`一对句子的语义相似程度`，并给出一个连续的分数（例如1到5分）
    - 重排序 (Reranking)：对一个已经初步检索出的文档列表进行二次优化排序，使得`最相关的文档排在最前面`
    - 分类 (Classification)：将单个文本（如电影评论、新闻文章）划分到预定义的类别中（如“正面/负面”、“体育/科技”）。特征分的好，后边类别结果就好
    - 聚类 (Clustering) ： 在没有任何预设标签的情况下，将一组文本自动地分成若干个有意义的群组，使得同一组内的文本语义相似，不同组间的文本语义差异大。
    - 对分类 (Pair Classification)： 判断一对文本（句子或段落）是否具有某种特定关系，通常是二分类问题，例如 “是否是重复问题”、“是否是转述关系”。
    - 双语挖掘 (Bitext Mining) ：从两种不同语言的大量句子中，找出互为翻译的句子对。对于机器翻译至关重要。
    - 摘要 (Summarization) ：这个任务比较特殊，它不是让模型生成摘要（即不是生成式的摘要），而是评估一个机器生成的摘要与人工撰写的参考摘要之间的`语义相似度`。



> 通过 MTEB 榜单，可以清晰地看到不同模型（如 BGE 系列, GTE, Jina 等）在不同任务类型上的性能表现。
例如，某些模型在检索任务上表现优异，而另一些则可能在聚类或分类任务上更具优势。这有助于我们根据具体应用场景，做出初步的模型筛选。
{: .prompt-info }


### Embeeding 的作用
* Embedding 可帮你做检索、排序、分类等，用于知识库，在知识库里查找一些信息能否找的对，靠的就是 Embeeding 向量，理解的好就找到准确率高


### 向量维度对模型性能的影响
* 向量维度直接影响模型的表达能力、计算开销和内存占用。
    - 高维度 (如 1024, 4096): 编码更丰富、语义更细致，适用于需要深度语义理解的复杂场景，如大规模、多样化
    的信息检索，或者细粒度的文本分类。但`计算成本更高，所需存储空间更大`。
    - 低维度 (如 256, 512): 计算速度快，内存占用小，更适合计算资源有限，或实时性要求高的场景，比如移动端。
    
* 如果把向量从 768 维拉长到 1024 维，检索指标提高不到1%，但内存要多占约 35%，是否还要升维？
    - 不需要，性价比低
    - 反过来，若压缩到 768 维后，指标下降超过 5% => 说明信息损失大，值得使用更高维度


> 大模型也可做 Embeeding 但是成本(钱+时间)很高
{: .prompt-info }



### Jina Embedding
* 由 Jina AI（官网 jina.ai）开发，公司总部位于德国柏林，专注于开源多模态搜索与向量化技术
* [jina-embeddings-v4](https://modelscope.cn/models/jinaai/jina-embeddings-v4) 是一个多模态和多语言检索的通用嵌入模型，特别适合用于复杂的文档检索，包括包含图表、表格和插图的视觉丰富文档。


![image](/assets/img/ai/start/jina.png)


> Jina Embedding 具有灵活的嵌入大小，默认情况下，密集嵌入为 2048 维，但可以截断到最低 128 维，性能损失较小。
{: .prompt-info }


#### 神奇的 `俄罗斯套娃`
* Jina-embeddings 训练时使用了一种特殊技术（Matryoshka Representation Learning, MRL）=> 俄罗斯套娃
生成完整向量：模型总是先在内部生成一个最完整、维度最高（比如 2048 维）的向量。
    - 按需截断：这个长向量有一个非常神奇的特性，它的前 128 维、前 256 维、前 512 维……本身就是一组高质量的、独立的、可以正常使用的短向量。
    

> 用户指定： 当调用模型时，可以通过 embedding_size 告诉模型 `我这次只需要前512维就够了` => 模型就会截断向量，只返回前 512 维
{: .prompt-info }


#### `俄罗斯套娃` 场景
* 场景 1：社交媒体情感分析
    - 社交媒体的文本短，实时性要求高，计算资源有限 => 可以使用128维
    - 在索引所有社交媒体评论时，都调用模型请求 128 维的向量。
    - 当用户进行查询时，您也同样将查询文本转换为 128 维的向量，然后进行比较。
* 场景 2：投资分析报告（要求高就用维度高的）
    - 投资分析、公司财报包含大量专业术语和细节（如风险提示、前瞻性声明），精准理解至关重要 => 2048 维。
    - 在索引所有投资分析报告时，都请求 2048 维的向量。查询语句也同样生成 2048 维的向量进行匹配。


> 动态调整维度是Jina-embedding模型赋予开发者的一个强大选项
{: .prompt-info }


### Embeeding 与大模型结合 
* Embeeding 算是大模型的一个小弟，用于辅助筛语料。
大模型的短板是其窗口，无法接收无限的数据，需要一个小弟（Embeeding）帮它来做（相似度）过滤，筛出的数据在给大模型


### 单语言与多语言 Embedding 模型的选择
* 单语言模型: 如 BGE-large-zh，专门针对单一语言（如中文）进行训练，
    - 例如：为电商平台开发一个智能客服问答系统。
    - 目标：系统需要能精准理解用户使用中文提出的问题：并从FAQ知识库中匹配最相关的答案。
        - 我的订单何时能送达？
        - 这个商品有保修吗？
        - 如何办理退换货？


* 多语言模型: 如 m3e-base 或 multilingual-e5-large 能够处理多种语言的文本，并将它们映射到统一的语义空间中。
    - 场景：为一个国际连锁酒店集团，建立全球客户评论分析系统。
    将来自世界各地的评论，按主题（如：客房清洁度、员工服务、地理位置）进行自动分类，无论评论是用英文、日文、西班牙文还是中文写的。
    - 总部的经理可以用英文查询 “Loud music at night”，系统需要能同时找出写着“夜に音楽がうるさい”的日文评论和“晚上音乐很吵”的中文评论。

> 多语言 Embedding 的优势是能将不同语言的文本映射到统一的语义空间。例如 `clean room`、`部屋が綺麗`和`干净的房间`的向量在空间中会非常接近。
=> 跨语言的聚类分析和检索才能实现。
{: .prompt-info }


### 如何选择适合的 Embedding 模型，依靠 MTEB 就可以？
* 模型选型是一个系统的过程，不能仅依赖于公开榜单。包括以下关键步骤：
    - 明确业务场景与评估指标: 首先定义核心任务是检索、分类还是聚类？并确定衡量业务成功的关键指标，如搜索召回率 (Recall@K)、准确率 (Accuracy) 或 NDCG。
    - 构建`黄金`测试集(建立自己的金标准): 准备一套能真实反映您业务场景和数据分布的高质量小规模测试集。比如，构建一系列`问题-标准答案`对 => 评估模型好坏的`金标准`。
    - 小范围对比测试 (Benchmark): 从 MTEB 榜单中挑选几款排名靠前且符合需求（如语言、维度）的候选模型。使用 `黄金`测试集，对这些模型进行评测。


> Embedding 模型的选择属于综合评估，即结合测试结果、模型的推理速度、部署成本 => 做出最终决策
{: .prompt-info }




## 向量数据库：AI 时代的核心记忆体
* 与传统的关系型数据库不同，向量数据库用于存储和查询由非结构化数据（如文本、图片、音视频）转化而来
的高维向量嵌入（Embeddings）。这些向量在多维空间中的距离代表了原始数据的语义相似度。
因此，。


### 向量数据库的核心价值
* 为大模型提供长期记忆： 弥补LLM上下文窗口（Context Window）长度限制和知识更新延迟的问题。
* 实现私有知识库的问答与搜索：将企业内部文档、产品信息等转化为向量，实现基于语义的智能检索。
* 赋能推荐系统、以图搜图等多种应用：通过计算用户、物品的向量相似度，提供更精准的推荐。


> 向量数据库是管理 Embeeding 的。向量数据库的核心能力是高效的相似性检索查找
{: .prompt-info }


### 常见的向量数据库
1. FAISS `轻量级`
    - 特点：由 Facebook 开发，专注于高性能的相似性搜索，适合大规模静态数据集
    - 优势：检索速度快，支持多种索引类型
    - 局限性：主要用于静态数据，更新和删除操作较复杂
2. Elasticsearch `全面`
    - 特点： 强大的分布式搜索和分析引擎，将向量搜索（k-NN）作为其众多功能之一
    - 优势： 具备业界领先的混合搜索能力，可以无缝结合传统的关键词搜索和向量语义搜索

3. Milvus `分布式，海量数据`
    - 特点：开源，支持分布式架构和动态数据更新
    - 优势：具备强大的扩展性和灵活的数据管理功能

4. Pinecone `云原生`
    - 特点：托管的云原生向量数据库，支持高性能的向量搜索
    - 优势：完全托管，易于部署，适合大规模生产环境


### 向量数据库 VS 传统数据库的
1. 数据类型
    - 传统数据库：存储结构化数据（如表格、行、列）
    - 向量数据库：存储高维向量数据，适合非结构化数据
2. 查询方式
    - 传统数据库：依赖精确匹配（如`=、<、>`）
    - 向量数据库：基于相似度或距离度量（如欧几里得距离、余弦相似度）
3. 应用场景
    - 传统数据库：适合事务记录和结构化信息管理
    - 向量数据库：适合语义搜索、内容推荐等需要相似性计算的场景
    
    
### Thinking：如何将数据导入向量数据库？
1. 数据清洗与准备: 确保原始数据（如文本文档、图片）的质量，进行必要的预处理
2. 数据向量化（Embedding）使用预训练的 Embedding Model 将原始数据转换成向量
    - 文本：可使用 bge-m, Qwen3-Embedding, Jina-Embedding 等模型
    - 图片：可使用 CLIP, ResNet 等模型
    - 选择合适的模型至关重要，它直接决定了向量的质量和后续检索的效果

3. 数据与元数据一同导入：将生成的向量及与其关联的元数据（Metadata）一同存入向量数据库
    - 向量（Vector）： 生成的 Embedding 数字数组。
    - 唯一ID（ID）： 用于唯一标识每个数据点，方便后续的更新或删除。
    - 元数据（Metadata）：`描述向量的附加信息，是实现高级检索的关键`。例如：
        - 文本来源的文件名、章节、URL
        - 商品的类别、品牌、价格
        - 图片的创建日期、作者



### Thinking：如何将 Embedding 和元数据一起存储在 FAISS ？
* FAISS 本身只存储和检索向量，不存储元数据。=> 我们需要在 FAISS 之外维护一个元数据的 “查找表”，并通
过向量在 FAISS 中的唯一ID将两者关联起来。
* 最直接有效的方法是使用 FAISS 的 IndexIDMap => 允许我们为每个向量指定一个自定义的、唯一的 64 位整数 ID。
然后，可以用这个ID作为元数据存储的键。



### Summary（实现步骤）
1. 准备数据：创建示例文本和对应的元数据
2. 生成向量：基于百炼 text-embedding-v4 生成每个文本的向量
3. 创建元数据存储：使用一个简单的 Python 列表来存储元数据。列表的索引将作为每个数据点的唯一ID
4. 构建 FAISS 索引：
    - 使用 faiss.IndexFlatL2 创建一个基础的索引 => 这里使用 L2 距离（欧氏距离）进行精确搜索
    - 用 faiss.IndexIDMap 将基础索引包装起来 => 这样就可以添加带有自定义 ID 的向量了
5. 添加数据到索引：将生成的向量和对应的 ID（即元数据列表的索引）添加到 IndexIDMap 中
6. 执行搜索：
    - 对一个新的查询文本生成向量
    - 在 FAISS 索引中搜索最相似的向量
    - FAISS 会返回最相似向量的ID
7. 检索元数据：使用返回的 ID，从元数据存储中查找到原始文本和元数据


#### Thinking：如何将 metadata 元数据管理的更健壮？
* 在更复杂的生产环境中，可以将 metadata_store 这个简单的列表替换为更健壮的存储方案：
    - 键值数据库（如 Redis）：通过 ID 快速查询，性能极高。
    - 关系型数据库（如 PostgreSQL）：可以存储更复杂的结构化元数据。
    - 文档数据库（如 MongoDB）：非常适合存储JSON格式的元数据。


> FAISS 专注于其最擅长的高速向量检索，而元数据的存储和管理可以交给专业的数据库系统负责，实现了架构上的解耦和高效协同。
{: .prompt-info }


![image](/assets/img/ai/start/elibvs.png)



## Q&A
### 1 向量数据库和 RAG 之间的关联
* 向量数据库 => 保存了 embedding 向量，通过数学计算，判断 query 向量和哪个片段的向量更接近 => 用于筛选 Top5 的片段 => 放到 LLM 上下文中，进行 RAG

### 2 Embeeding 模型 VS 大模型 
* Embeeding 模型（向量模型）：帮你做特征提取。它无法帮你写文章等
* 大模型：帮你回答问题的

### 3 多语言 Embedding 如何训练
* 根据下游任务进行训练，例如本篇中 Word2Vec 下游任务就是找相邻的单词
* MTEB 的 8 大分类也是下游任务
* 下游任务是训练两者关系的桥梁
* Word2Vec、BERT（Transformer 出来前） 可以训练语料


### 4 faiss 安装

```shell
# 安装 CPU 版本
# pip install faiss-cpu 

# 安装 GPU 版本
# pip install faiss-gpu
```
