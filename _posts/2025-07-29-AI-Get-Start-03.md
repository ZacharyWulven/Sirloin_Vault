---
layout: post
title: 03-Embeddings 和向量数据库
date: 2025-07-29 16:45:30.000000000 +09:00
categories: [AI, AIStarted]
tags: [AI, AIStarted]
---


## Embeddings 和向量数据库
* Embeddings 和向量数据库：用于 RAG 知识库


### 传统语义特征计算方法
#### 为酒店建立内容推荐系统

* 基于用户选择的酒店，推荐相似度高的 Top10 个其他酒店
    - 方法：计算当前酒店（描述信息）特征向量与整个酒店特征矩阵的余弦
    
    
![image](/assets/img/ai/start/e_x.png)


* 句子 A 向量：[1, 1, 2, 1, 1, 1, 0, 0]
* 句子 B 向量：[1, 1, 1, 0, 1, 2, 1, 1]


![image](/assets/img/ai/start/e_x2.png)

    
> 向量就是在某些特征上表达
{: .prompt-info }


* 余弦相似度：
    - 通过测量两个向量的夹角的余弦值来度量它们之间的相似性。
    - 判断两个向量⼤致方向是否相同，方向相同时，余弦相似度为 1；两个向量夹角为 90° 时，余弦相似度的值为 0，方向完全相反时，余弦相似度的值为 -1。
    - 范围：两个向量之间夹角的余弦值为 [-1, 1]；1 为完全一致，-1 为完全相反

     
> 分词：将句子拆分为分词
{: .prompt-info } 
    
   
### N-Gram：N 元语法
* 上边对两个句子向量特征问题？
    - 问题在于对特征的理解过于简单了，只看有还是没有，而没有考虑顺序等
    - 上边只是一种维度，即 `N=1 unigram`, 称为一元语法


* N-Gram：N 元语法:
    - 基于一个假设：第 n 个词出现与前 n-1 个词相关，而与其他任何词不相关
    - N=1 unigram; N=2 bigram; N=3 trigram
    - N-Gram 指的是给定一段文本，其中的 N 个 item 的序列
    - 比如文本：A B C D E，对应的 Bi-Gram（组合）为 A B, B C, C D, D E
    - 当一阶特征不够用时，可以用 N-Gram 做为新的特征。比如在处理文本特征时，一个关键词是一个特征，但有些情况不够用，需要提取更多的特征


> 采用 N-Gram => 可以理解是相邻两个关键词的特征。如何了解事物的特征表达？N-Gram 就是最基本的一种方式
{: .prompt-info }



### TF-IDF 
* `TF-IDF = TF * IDF`

* TF:（Term Frequency）, `词频：TF = 单词次数 / 文档中总单词数`
    - 一个单词的重要性和它在文档中出现次数呈比例
    - 例如：`hotel` 单词在所有酒店描述中都出现过，那个这个 `hotel` 就不重要了

* IDF（Inverse Document Frequency）：逆向文档频率，即可理解为区分度、重要性
    - 一个单词在文档中的区分度。这个单词出现在文档中次数越少，区分度越大，IDF 越大
    - 公式：`IDF = log(文档总数 / (单词出现的文档数 + 1))`


![image](/assets/img/ai/start/h.png)


### 推荐系统原理

#### 基于内容的推荐：
* Step1：向量化，即对酒店描述（每个商品）进行特征提取
    - N-Gram：提取 N 个连续字的集合，作为特征的元素
    - TF-IDF：按照 (min_df, max_df) 提取关键词，并生成 TFIDF 矩阵

* Step2：预选计算酒店之间的相似度矩阵
    - 余弦相似度

* Step3：对于指定的酒店，选择相似度最大的 Top-K 个酒店进行输出


> 两者像不像是看向量之间的对比，向量是特征上的表达。中文分词器：Python 库 jieba
{: .prompt-info } 


## Word Embedding (Google 推出)

### 思考：试想一下上边酒店推荐有 26879 维度特征
* N-Gram + TF-IDF 的特征表达会让特征矩阵非常稀疏（很多值为 0），计算量大，有没有更适合的方式？
* 有没有更好的方式让它不稀疏，答案是 `Embedding` 


### 什么是 Embeddings
* `Embedding` 是一种降维度策略，将不同特征转换为维度相同的向量
    - 压缩到一个向量中，使得数据不为 0 更容易计算。向量是通用技术，万事万物都可向量化。也可对人进行 Embeddings
    - `Embedding` 本质：即把数据放到一个密闭空间中（大体积 => 小体积），方便存储和管理信息



![image](/assets/img/ai/start/e_word.png)



### Word Embedding 例子

![image](/assets/img/ai/start/w_e.png)


![image](/assets/img/ai/start/w_e2.png)


* 每个单词都是 50 维，有意思是 `king - man + woman 与 queen 近似`


#### Embedding 工具：Word2Vec
* 这个工具是训练出来的
* 理论：
    - 通过 Embedding，把原先词所在空间映射到一个新的空间中去，使得语义上相似的单词在该空间内距离相近。
    - Word Embedding => 学习隐藏层的权重矩阵
    - 输入测是 one-hot 编码
    - 隐藏层的神经元数量为 hidden_size（Embedding Size）
    - 对于输入层和隐藏层之间的权值矩阵 W， 大小为 [vocab_size, hidden_size]
    - 输出层为 [vocab_size] 大小的向量，每一个值代表着输出一个词的概率

* 通俗易懂解释：
    - 把特征压缩，以前特征有和没有用 `1/0` 表达, 
    - 例如大不列颠百科全书：这本书假如一共一万个单词，TF 词频铺开，有就为 1，没有就为 0，
    - 对于 king 这个词，某一位为 1，其他都是 0，这时一万维中绝大部分是 0，数据很稀疏
    - 那么可想办法从一万维度压缩（Embedding）到 300 维度，减少 0 方便后边计算
    - 虽然 Embedding 压缩可能造成信息丢失，但可用到原来 3% 的存储空间占到 99% 的信息量，这样收益较大
    
    
* Word2Vec 两种模式：让其预测一个单词两边的邻居是什么? 或猜邻居的概率

* Word2Vec Embedding => 1 万维度到 300 维度

![image](/assets/img/ai/start/e_h.png)



#### 训练词向量工具 Gensim

```shell
$ pip install gensim
```

* 可以从非结构化文本中，无监督地学习到隐层的主题向量表达
* 每一个向量变换的操作都对应着一个主题模型
* 支持 TF-IDF，LDA, LSA, word2vec 等多种主题模型算法
* 使用方式：
    - 建立词向量模型：word2vec.Word2Vec(sentences) window 句子中当前单词和被预测单词的最大距离 
    - min_count 需要训练词语的最小出现次数，默认为 5 
    - size 向量维度默认为 100
    - worker 训练使用的线程数，默认为 1 即不使用多线程
    - 模型保存 model.save(fname)
    - 模型加载 model.load(fname)


> 这个工具使得计算机对单词是用向量的方式去理解的
{: .prompt-info } 


* 如果训练的好，最后结果可能会有些规律，例如前边的 `king - man + woman 与 queen 近似`

> 小结：隐藏层就是 Embeeding，可以理解为是一个向量字典。用神经网络压缩到你设计的网络结构（以 Word2Vec 为例，即 300 维度的隐藏层），让它去学习然后完成输出任务。原来是一万维度输入 => 输出层。`Embeeding` 后是基于 300 维度的隐藏层 => 输出层
{: .prompt-info } 



> 计算机背后的`压缩的特征` 即 `Embeeding`
{: .prompt-info }


### QA
#### 1 向量数据库和 RAG 之间的关联
* 向量数据库 => 保存了 embedding 向量，通过数学计算，判断 query 向量和哪个片段的向量更接近 => 用于筛选 Top5 的片段 => 放到 LLM 上下文中，进行 RAG

#### 2 Embeeding 模型 VS 大模型 
* Embeeding 模型（向量模型）：帮你做特征提取。它无法帮你写文章等
* 大模型：帮你回答问题的


## Embeeding 模型选择

### Embeeding 的作用
* Embedding 模型将文本等离散数据转换为低维、稠密的向量，捕捉其语义信息。向量空间中的距离（如余弦相似度）可反映文本间的语义相似度。
    - 维度：2 的 N 次方
* Embedding 模型强度远远小于大模型，它只是压缩信息到一个空间中，它没有做生成式的 AI 


#### [MTEB](https://huggingface.co/spaces/mteb/leaderboard) (Massive Text Embedding Benchmark) 
* 是一个全面的评测基准，它涵盖了分类、聚类、检索、排序等 8 大类任务和 58 个数据集。
* 通过 MTEB 榜单，可以清晰地看到不同模型（如 BGE 系列, GTE, Jina 等）在不同任务类型上的性能表现。
    - 例如，某些模型在检索任务上表现优异，而另一些则可能在聚类或分类任务上更具优势。这有助于我们根据具体应用场景，做出初步的模型筛选。

> Embedding：把一个信息（例如单词）向量化，进行数据抽取
{: .prompt-info }







## 向量数据库：AI 时代的核心记忆体
* 向量数据库是管理 Embeeding 的
* 向量数据库是近似查找
