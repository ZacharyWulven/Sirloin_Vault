---
layout: post
title: 02-DeepSeek 使用与 Prompt 工程
date: 2025-07-23 16:45:30.000000000 +09:00
categories: [AI, AIStarted]
tags: [AI, AIStarted]
---


## DeepSeek 使用与 Prompt 工程


## 蒸馏
* 可以想象成是老师教学生，例如 `DeepSeek-R1-Distill-Qwen-1.5B`,
    - 老师就是 `DeepSeek-R1, 学生是 `Qwen`，老师把它自己的数据蒸馏给学生，让学生得到老师一定的能力
    - 虽然跟老师能力有差距，但相对于自己的能力已经有了大幅度提升
    


## DeepSeek 的创新

### 1 MLA（Multi-Head Latent Attention）(缓存相关，性价比高)
* 在推理过程中底层基于 `Transformer` 模型架构中的多头注意力机制（MHA，Multi-Head Attention），MHA 是 `Transformer` 架构的核心组件和关键创新


> 多头注意力机制（MHA，Multi-Head Attention）：例如，你问 hello 时候，可能也有很多人问，这时候会在 GPU 中缓存，就导致会占用一些带宽。传统多头注意力的键值（KV）是 `全量的方式`, 这就对计算效率造成了较大阻碍
{: .prompt-info }


* DeepSeek 引入了 MLA，一种通过低秩键值联合压缩的注意力机制，来显著减小 KV 缓存的同时提升计算效率
    - 即不需要 100% 的缓存，只需要用 5% 的空间缓存 98% 的信息，实际上做了个压缩
    - 类似你手机拍的照片，全量大小大概是 4M，但 Chat App 一般不用原图，会用压缩的图
    - MLA 对精度（键值）有一些损失，但是它便宜，性价比高
    


### 2 DeepSeek-MoE 混合专家架构 (性价比高)

### MoE：即提前储备了很多专家，但真正用的时候不用所有的专家，只用部分专家
* Qwen3-Coder-480B-A35B：可以理解为它就是一个 35B 的模型
    - 480B：完整尺寸的模型参数大小
    - A35B：训练时激活的参数量是 35B
    - 有点像一个知识库过滤的意思


> MoE 比 Dense(即激活全部专家，例如 Qwen3-Coder-480B-A35B 就是激活 480B) 训练难度大
{: .prompt-info }


* DeepSeek-V3 使用了 61 个 MoE (Mix of Expert 混合专家) block，虽然总参数量很大，但每次训练或推理时只激活了很少链路，训练成本大大降低，推理速度显著提高。
* MoE 类比为医院的分诊台，在过去所有病人都要找全科医生，效率很低。
    - 但是 MoE 模型相当于有一个分诊台，将病人分配到不同的专科医生那里
    - DeepSeek 在这方面也有创新，之前分诊是完全没有医学知识的保安，而现在用的是有医学知识的本科生来处理分流任务

![image](/assets/img/ai/start/moe.png)



### 3 混合精度框架 (性价比高)
* 全精度是 `32Bit` 保存一个参数的数值，参数就是权重，大模型权重如果都用 `32Bit` 来保存，它是没有压缩的
    - 优点：很准确
    - 缺点：数据量大，推理速度慢
* 优化方案是降低精度，例如 `16Bit` 或 `8Bit`
    
* 混合江都框架：即动态调整的进行保存，重要的关键的位置用全精度。不重要位置用 `8Bit` 精度，不够用了再用 `16Bit` 或 `32Bit`

![image](/assets/img/ai/start/mix_f.png)


## Summary: 

### 为什么 DeepSeek 计算速度快，成本低？

#### 架构设计方面
1. MoE 架构：在推理时仅激活部分专家，避免了激活所有参数带来的计算资源浪费
2. MLA 架构：通过低秩 KV 矩阵，减少了显存消耗


#### 训练策略方面
1. 多 token 预测（MTP）目标：在训练过程中采用多 token 预测目标，即在每个位置上预测多个未来 token，增加了训练信号的密度，提高了数据效率
2. 混合精度训练框架：在训练中，对于占据大量计算量的通用矩阵算法（GEMM）操作，采用了 FP8 精度执行。同时，通过细粒度量化策略和高精度累积过程，解决了低精度训练中出现的量化误差问题



### 为什么 DeepSeek-R1 推理能力强大?

* 1 强化学习驱动（Think 过程）（重要）: DeepSeek-R1 通过大规模强化学习技术显著提升了推理能力。在数学、代码和自然语言推理等任务上表现出色，性能与 OpenAI 的 o1 正式版相当（OpenAI 的 o1 也是推理模型）
    -  强化学习的作用：训练大模型，结合少了 `SFT`，引入少量高质量监督数据（如数千个 CoT 示例）进行微调，提升模型初始推理能力，再通过 `RL 强化学习（Reinforcement Learning）` 进一步优化，最终达到与 OpenAI o1 相当的性能
* 2 长链推理技术（CoT）：DeepSeek-R1 采用长链推理技术，其思维链长度可达数万字，能够逐步分解复杂问题，通过多步骤的逻辑推理来解决问题
    - CoT 让 AI 模型逐步分解复杂问题，比如在智能客服，市场分析报告等领域
    
    
### 训练过程

#### Alpha-Go
* 之前有个 `Alpha-Go-Zero`，它用强化学习，进行自我对弈，与李世石对弈前，它自我对弈了 3000 万盘

> 版本号为 `Zero`，即不需要人干预，自我对弈，即机器自己给自己反馈
{: .prompt-info }


#### DeepSeek-R1-Zero（纯强化学习）
* `<think>` 草稿纸：think 推理过程
* `<answer>` 答案纸：最终结果

> DeepSeek-R1-Zero 首次验证了纯强化学习在 LLM 中能显著增强推理能力的可行性，即无需 `SFT`，仅通过 `RL` 即可激励模型学会长链推理和反思。
提出了多阶段训练策略（冷启动 SFT->RL->SFT->全场景 RL），有效兼顾准确率与可读性，产出 DeepSeek-R1
{: .prompt-info }


#### DeepSeek-R1
* 在 DeepSeek-V3（文科生）基础上训练它的理科能力，DeepSeek-R1-Zero（理科生，有一些理科能力的标记）

1. 冷启动 SFT 阶段（DeepSeek-R1-Zero 对 V3 进行监督学习），DeepSeek-R1-Zero 先生成几千条数据作为 Seed
2. 模型生成推理数据 VS 非推理数据
3. 数据合并
4. 用 80w 语料进行训练（SFT）
    - 方向一：`DeepSeek-V3 + 80w 语料`，进行二次训练和强化学习探索 => `DeepSeek-R1` 版本
    - 方向二：`Qwen3-7B + 80w 语料` => `DeepSeek-R1-Distill` 蒸馏版本


![image](/assets/img/ai/start/sft.png)



## 蒸馏
* 大模型（老师）学到的知识压缩进小模型（学生），让后者在体量更小，推理更快的同时保留主要能力
* 常用方法：
    - 软标签蒸馏：用老师输出的概率分布训练学生
    - 隐藏层蒸馏：对齐中间表示
    - 数据增强：教师生成伪数据再训练
    
    
    
## 量化（将模型体积变小）
* 指通过降低模型参数的存储精度（如从 32 位浮点数降至 8 位整数），以减少模型体积，加速推理并降低计算资源需求
* 核心原理：
    - 精度压缩：减少每个参数的比特数，例如 FP32→INT8 可减少 75% 存储需求。
    - 误差补偿：如 GPTQ 采用逐层量化并调整剩余权重，以减少信息损失。
    - 硬件加速：低精度计算（如INT8）在 GPU/TPU 上通常比 FP32 更快
    
    
![image](/assets/img/ai/start/liang.png)


> 量化越低，模型越小、越快，但越可能在需要精细数值推理或知识细节的任务上出错。
{: .prompt-info }



![image](/assets/img/ai/start/liang2.png)



## 不同尺寸的 LLM（DeepSeek）
* 小模型（1.5B-14B）
    - 响应快、硬件需求低，但基础能力薄弱（如7B模型在基础文本生成任务中表现不稳定，甚至“不及格”）无法胜任复杂任务
    - 7B 是个人版本

* 中尺寸（32B及以上）
    - 性能接近满血版（32B约实现671B的90%性能），可满足专业领域需求，但本地化部署成本较高（需64GB内存、80GB显存等）
    - 32B 企业入门级， 10万人民币左右
    - 70B，65万

    
* 满血版（671B）：200-300万
    - 性能最强，适合超大规模AI研究、AGI 探索，但部署成本极高（需多节点分布式训练，硬件需求极高）
    - MoE-QWen3 性价比比 671B 高

![image](/assets/img/ai/start/size.png)


> 这里只有 671B 是正式版，其他都是蒸馏版本
{: .prompt-info }



## 不同尺寸的 LLM（Qwen3）

* 不同尺寸的 Qwen3 模型适用于不同场景：
    - 0.6B/1.7B 适合本地测试、科研或边缘设备（如工控机）部署，
    - 4B 适用于手机端应用，
    - `8B` 适用于电脑或汽车端的对话系统、语音助手等，
    - 14B/`32B` 适合企业复杂任务落地，
    - 30B-A3B 和 `235B-A22B` 则分别适合云端高效部署和旗舰级高性能场景（如数学证明、代码生成等）235B-A22B在基准测试中可与 DeepSeek-R1、Grok-3 等顶尖模型竞争 。
* Qwen3 模型支持“思考模式”（逐步推理）和“非思考模式”（快速响应），并支持 119 种语言和方言，具备更强的 Agent 能力和原生 MCP 支持 。

* AutoDL + 4090 部署 8B 


> DeepSeek-V3 就是文科生。DeepSeek-R1 是理科生（缺点是思考东西太多）。解决幻觉方法：重新开一个新的对话，
{: .prompt-info }


## 私有化部署 DeepSeek 模型

* [modelscope](https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)


### 1 使用 Vllm 部署
* 线上使用 vllm 部署


### 2 pythone 库部署



### 3 使用 Ollama 部署 [Ollama](https://ollama.com/)
* 下载安装 Ollama 后 

```shell
# ollama pull deepseek-r1:1.5b

# 删除模型
# ollama rm deepseek-r1:1.5b


# 启动模型
# ollama run deepseek-r1:1.5b
```

## Prompt 提示词工程

### Prompt原理
* GPT 在处理 Prompt 时，GPT 模型将输入的文本（也就是 Prompt）转换为一系列的词向量。然后，模型通过自回归生成过程逐个生成回答中的词汇。在生成每个词时，模型会基于输入的 Prompt 以及前面生成的所有词来进行预测。
* 这个过程不断重复，直到模型生成完整的回答 
* 一个有效的 Prompt 可以：
    - 提升 AI 模型给出的答案的质量
    - 缩短与 AI 模型的交互时间，提高效率
    - 减少误解，提高沟通的顺畅度


![image](/assets/img/ai/start/size.png)


### 提示词策略差异

#### 推理模型
* 提示语更简洁，指需要明确任务目标和需求，因为模型已经内化了推理逻辑
* 无需逐步指导，模型会自动生成结构化推理过


#### 通用模型
* 需要显示引导推理步骤，比如通过 CoT 提示，否则可能会忽略关键逻辑
* 依赖提示词补偿能力短板，比如要求分步骤思考，提供 few-shot 参考示例等


### 模型选择
* 复杂任务用推理模型，其他或创意类的用通用模型
 

### 提示词设计
* 通用模型：结构化、补偿性引导 => 缺什么，补什么
* 推理模型：简洁指令、聚焦目标、信任其内化能力 => 要什么，直接说



### Prompt 编写原则

1. 明确目标：清晰定义任务，以便模型理解
2. 具体指导：给予模型明确的指导和约束
3. 简洁明了：使用简练、清晰的语言表达 Prompt
4. 适当引导：通过示例或模版或问题边界引导模型
5. 迭代优化：根据输出结果，持续调整和优化


* 一些有效做法：
    - 强调，可以适当的重复命令和操作
    - 给模型一个出路，如果模型可能无法完成，告诉它说"不知道"
    - 尽量具体，对于专业性要求强的，少留解读空间（在你的专业领域中，把它看成孩子）



### Prompt 编写框架（重要性排序）
* Prompt的组成：
    1. 任务（Task）: 始终以动词开始任务句子（如"生成"，"给予"，"写作"等），明确表达你的最终目标，可以有多个。
    2. 上下文（Context）: 提供上下文背景、所处环境等。
    3. 示例（Exemplars）: 提供具体的例子，用于可以提高输出质量。有时候我们没有示例，这时可以让GPT帮忙生成示例。
    4. 角色（Persona）: 你希望AI扮演的角色。可以是具体的人，也可以是虚构的角色。
    5. 格式（Format）: 你希望输出的格式，如表格、列表、段落等。
    6. 语气（Tone）: 指定输出的语气，如正式、非正式、幽默等。


> 重要性排序 `任务 > 上下文 > 示例 > 角色 > 格式 > 语气`。写 prompt 是一个不断尝试和调整的过程
{: .prompt-info }

    

### Prompt 编写技巧

1. 限制模型输出的格式
2. 使用分隔符，例如用三个引号把文本括起来（"""text1""" """text2"""）
3. 提供示例，模版
4. CoT（Chain of Thought，思维链）通过将复杂任务分解为多个简单的步骤，帮助模型系统地思考并解决问题。
5. 面向不同的角色进行讲解。例如 把我当做五岁小朋友一样，向我解释超导体。
