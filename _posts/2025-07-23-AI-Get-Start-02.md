---
layout: post
title: 02-DeepSeek 使用与 Prompt 工程
date: 2025-07-23 16:45:30.000000000 +09:00
categories: [AI, AIStarted]
tags: [AI, AIStarted]
---


## DeepSeek 使用与 Prompt 工程


## 蒸馏
* 可以想象成是老师教学生，例如 `DeepSeek-R1-Distill-Qwen-1.5B`,
    - 老师就是 `DeepSeek-R1, 学生是 `Qwen`，老师把它自己的数据蒸馏给学生，让学生得到老师一定的能力
    - 虽然跟老师能力有差距，但相对于自己的能力已经有了大幅度提升
    


## DeepSeek 的创新

### 1 MLA（Multi-Head Latent Attention）(性价比高)
* 在推理过程中底层基于 `Transformer` 模型架构中的多头注意力机制（MHA，Multi-Head Attention），MHA 是 `Transformer` 架构的核心组件和关键创新


> 多头注意力机制（MHA，Multi-Head Attention）：例如，你问 hello 时候，可能也有很多人问，这时候会在 GPU 中缓存，就导致会占用一些带宽。传统多头注意力的键值（KV）是 `全量的方式`, 这就对计算效率造成了较大阻碍
{: .prompt-info }


* DeepSeek 引入了 MLA，一种通过低秩键值联合压缩的注意力机制，来显著减小 KV 缓存的同时提升计算效率
    - 即不需要 100% 的缓存，只需要用 5% 的空间缓存 98% 的信息，实际上做了个压缩
    - 类似你手机拍的照片，全量大小大概是 4M，但 Chat App 一般不用原图，会用压缩的图
    - MLA 对精度（键值）有一些损失，但是它便宜，性价比高
    


### 2 DeepSeek-MoE 混合专家架构 (性价比高)

### MoE：即提前储备了很多专家，但真正用的时候不用所有的专家，只用部分专家
* Qwen3-Coder-480B-A35B：可以理解为它就是一个 35B 的模型
    - 480B：完整尺寸的模型参数大小
    - A35B：训练时激活的参数量是 35B
    - 有点像一个知识库过滤的意思


> MoE 比 Dense 训练难度大
{: .prompt-info }


* DeepSeek-V3 使用了 61 个 MoE (Mix of Expert 混合专家) block，虽然总参数量很大，但每次训练或推理时只激活了很少链路，训练成本大大降低，推理速度显著提高。
* MoE 类比为医院的分诊台，在过去所有病人都要找全科医生，效率很低。
    - 但是 MoE 模型相当于有一个分诊台，将病人分配到不同的专科医生那里
    - DeepSeek 在这方面也有创新，之前分诊是完全没有医学知识的保安，而现在用的是有医学知识的本科生来处理分流任务

![image](/assets/img/ai/start/moe.png)





### 3 混合江都框架 (性价比高)
* 全精度是 `32Bit` 保存一个参数的数值，参数就是权重，大模型权重如果都用 `32Bit` 来保存，它是没有压缩的
    - 优点：很准确
    - 缺点：数据量大，推理速度慢
* 优化方案是降低精度，例如 `16Bit` 或 `8Bit`
    
* 混合江都框架：即动态调整的进行保存，重要的关键的位置用全精度。不重要位置用 `8Bit` 精度，不够用了再用 `16Bit` 或 `32Bit`

![image](/assets/img/ai/start/mix_f.png)


## Summary: 为什么 DeepSeek 计算速度快，成本低？

#### 架构设计方面


### 为什么 DeepSeek-R1 推理能力强大?

#### 强化学习驱动（重要）
* DeepSeek-R1 通过大规模强化学习技术显著提升了推理能力。在数学、代码和自然语言推理等任务上表现出色，性能与 OpenAI 的 o1










#### 长链推理技术（CoT）




Alpha-Go-Zero 进行自我对弈，与李世石对弈前 它自我对弈了 3000 万盘


OpenAI 的 o1 也是推理模型



## 蒸馏的定义与方法

## 什么是蒸馏


## 量化





## 不同尺寸的 LLM（DeepSeek）

7B 是个人

32B 企业入门级， 10万人民币左右

70B 65万
671B 200-300万


MoE-QWen3 性价比比 671B 高


## 不同尺寸的 LLM（Qwen3）

* AutoDL 部署 8B 


DeepSeek-V3 就是文科生
DeepSeek-R1 是理科生（缺点是思考东西太多）

解决幻觉方法：重新开一个新的对话，


## 私有化部署 DeepSeek 模型


### 使用 Ollama 部署


线上使用 vllm 部署


## Prompt 提示词工程

Prompt 编写原则

* 简洁明了


### Prompt 编写框架（重要性排序）
* 重要性排序
    - 任务: 
    
    

### Prompt 编写技巧

1.
2. 使用分隔符，例如用三个引号把文本括起来
3. 示例
4. CoT（Chain of Thought）思维链
给一个示例，示例呈现的逻辑，



    
    







![image](/assets/img/ai/start/how_tow_train.png)


