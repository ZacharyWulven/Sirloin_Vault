---
layout: post
title: 11-视觉大模型与多模态理解 
date: 2025-08-24 16:45:30.000000000 +09:00
categories: [AI, AIStarted]
tags: [AI, AIStarted]
---


## 视觉大模型与多模态理解

## 1 VLM 在行业中的应用

### 图像视觉理解：
1. 通用：[Qwen-VL](https://github.com/QwenLM/Qwen2.5-VL)
  - Qwen-VL 泛化能力强，但可能识别不准，会有盲区
    - 盲区：以前的训练样本不均衡，所以如果你给一些小众的情况，它可能识别不出来
2. 文字识别 OCR：[PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR)
  - 无法回答问题，只能识别 

### 车辆事物要素提取
* Prompt = 以 JSON 格式提取信息；因为 JSON 格式以便后续填表

### [Qwen-VL 微调](https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune)
* 对于新发布车型模型没训练学习过，所以无法识别，可通过微调让它学会

* 微调的训练数据：问答对

* 使用 trainer.py 进行训练

```json
{
    "image": "images/001.jpg",
    "conversations": [
        {
            "from": "human",
            "value": "<image>\nWhat's the main object in this picture?"
        },
        {
            "from": "gpt",
            "value": "A red apple on a wooden table"
        }
    ]
}
```

* 训练显卡
  - A100，H100 适合做训练
  - GTX4090 适合做推理


> xxx.safetensors 文件大小的和就是需要的显存数量，但最好还是多留一些 Buffer 用于额外计算
{: .prompt-info }

## 2 [InternVideo2.5](https://modelscope.cn/models/OpenGVLab/InternVideo2.5)（视频理解SOTA）
* SOTA：即最好的，最先进的

### 视频基础模型
* InternVideo2，新型的视频基础模型（ViFM），在视频识别、视频文本任务和对话任务中取得了SOTA。
InternVideo2 的核心设计是渐进式训练方法，统一了掩蔽视频建模、跨模态对比学习和下一个 token 预测，
将视频编码器的规模扩大到 60 亿参数。在数据层面，通过语义分割视频并生成视频-音频-语音字幕，优
先考虑时空一致性，从而提高了视频和文本之间的对齐。

### InternVideo2 训练的三个阶段
1. 通过未掩蔽重建捕获时空结构
2. 与其他模态的语义对齐，视频、文本之间对齐
3. 通过下一个token预测增强其开放式对话能力

> 不同的训练阶段将引导该模型通过不同的前置任务捕捉不同层次的结构和语义信息。
{: .prompt-info }


### Google 训练集
* Kinetics-400 是一个大规模的视频数据集，专门用于动作识别和理解的研究。它由 Google DeepMind 创建。
Kinetics-400 包含了大量的视频剪辑，每个剪辑大约持续 10 秒钟，涵盖了400种不同的动作类别，比如日常生活活动（如做饭、跳舞）、
体育运动（如踢足球、打篮球）、社交互动（如握手）等。

### 预训练
* 对于 InternVideo2 的训练，强调数据中的时空一致性和标签质量。
数据集包含 4.02 亿数据条目，其中包括 `200万个视频、5000万个视频-文本对（来自 WebVid 和 InternVid）、5000 万个视频-音频-语音-文本对（InternVid2）和3亿个图像-文本对。`
* 对于 InternVid2，我们将视频语义分割成剪辑，并专注于使用三种模态：音频、视频和语音重新校准剪辑描述。我们首先
为这三种模态分别生成字幕。然后，单独的字幕被融合在一起，用于创建一个更全面的描述。

* 三个阶段
  1. 200万个视频进行无监督的学习
  2. 视频、文本，图片之间对齐
  3. 对话进行标注、理解



> 视频多模态注释系统的框架称为 VidCap，由四个主要组件组成：视频、音频和语音字幕，以及用于集成这些字幕的 LLM
{: .prompt-info }


> 未来大模型有时间序列的概念，有前后的记忆，有助于整体的理解
{: .prompt-info }


* [Tech Report](https://arxiv.org/abs/2501.12386)

> 对于一个模型重点看它的 Tech Report 里边有一些试验结果，能知道模型在哪方面更出色
{: .prompt-info }


> 模型有个 config.json 里边有写需要其他哪些模型
{: .prompt-info }


## 3 [MinerU](https://mineru.net/apiManage/usage)

* MinerU 专注于高效解析和提取复杂的 PDF 文档、网页和电子书，并将其转换为易于分析的 Markdown 或 JSON 格式。由 上海人工智能实验室OpenDataLab 团队 开发。

* 主要功能包括：
  - PDF 转 Markdown
    - 支持多模态 PDF（含图片、表格、公式等）的结构化转换
    - 自动去除页眉、页脚、脚注等干扰信息，保留标题、段落、列表等结构
    - 公式识别并转换为 LaTeX 格式，表格转换为 HTML 或 Markdown
  - 网页内容提取：从网页中剔除广告等干扰信息，精准提取正文、评论、视频文字等内容
  - 电子书转换：支持 epub、mobi、docx、pptx、chm、azw 等格式批量转 Markdown
  - 多语言 OCR：自动检测扫描版 PDF 和乱码，支持 84 种语言 的 OCR 识别


* 核心技术（小模型的叠加 ）
  - 布局检测：基于 LayoutLMv3 微调，识别文本、表格、图片等区域
  - 公式识别：使用 YOLOv8 检测公式，UniMERNet 模型转换 LaTeX
  - OCR 增强：采用 PaddleOCR 提高文本识别准确率
* 应用场景
  - 大模型训练：为书生· 浦语等模型提供高质量语料
  - 学术研究：提取论文、教材中的关键信息
  - 法律与金融：解析合同、研报等结构化数据

* MinerU 支持 CPU/GPU，兼容 Windows/Linux/Mac

### MinerU 使用
* [在线使用](https://mineru.net/OpenSourceTools/Extractor)
* [客户端](https://mineru.net/client)
* [API](https://mineru.net/apiManage)
  - Step1：申请 API Token（很快审核通过，约 10 分钟）
  - Step2：通过 API，提交 PDF，获取 task_id
  - Step3：通过 task_id，获取任务状态及内容
  - Step4：返回的 `full_zip_url` 为最终数据，里边的 `layout.json` 中的 `score` 表示置信度
* 本地化部署


### MinerU 一般什么情况下使用？
1. 解析 PDF 时候，做知识库时候
2. 网页抓取：需要对网页内容进行抓取，可以用 MinerU 进行解析


### MinerU 私有化部署（不强制需要 GPU）
* 企业文档要转化为 markdown

* Step1: 安装 magic-pdf
  - `pip install -U "magic-pdf[full]"`
* Step2: 下载依赖的 models
  - `python download_models.py`
  - 使用 modelscope 快速下载各种 models
* Step3: 推理 pdf 
  - `$ magic-pdf -p 三国演义.pdf -o ./output`
    - magic-pdf 是 PDF 的工具箱




## Q & A

### 1 表格用什么形式表示，对 AI 来说理解能力更强？
* marktable
* html 最好，因为 html 有很多标签


### 2 私有化部署模型
* 模型会及时更新到 modelscope
* 性能如果要求比较高，可以用 vllm 进行部署，有加速可多人使用
* Python 代码部署，则没有加速，


### 3 模型微调两种方式
1. 全量
2. 高效（常用）
